# Local sagemaker endpoint deployment - modify accordingly.
networks:
  sagemaker-local:
    name: sagemaker-local
services:
  algo-1-9z9sq:
    command: serve
    container_name: culmrfzoht-algo-1-9z9sq
    deploy:
      resources:
        reservations:
          devices:
            - capabilities:
                - gpu
              count: all
    environment:
      - MMS_MAX_REQUEST_SIZE=2000000000
      - MMS_MAX_RESPONSE_SIZE=2000000000
      - MMS_DEFAULT_RESPONSE_TIMEOUT=900
      - SAGEMAKER_PROGRAM=inference.py
      - SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/code
      - SAGEMAKER_CONTAINER_LOG_LEVEL=20
      - SAGEMAKER_REGION=ap-southeast-1
    image: 763104351884.dkr.ecr.ap-southeast-1.amazonaws.com/huggingface-pytorch-inference:2.1.0-transformers4.37.0-gpu-py310-cu118-ubuntu20.04
    networks:
      sagemaker-local:
        aliases:
          - algo-1-9z9sq
    ports:
      - 8080:8080
    stdin_open: true
    tty: true
    volumes:
      - ./models/Systran-faster-whisper-large-v2:/opt/ml/model
      - ./src/transcription_benchmarks/inference/systran_faster_whisper_large_v2:/opt/ml/model/code
